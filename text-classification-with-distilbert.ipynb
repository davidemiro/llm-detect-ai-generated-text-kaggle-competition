{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985ff08d",
   "metadata": {
    "papermill": {
     "duration": 0.004677,
     "end_time": "2024-01-17T22:31:26.288959",
     "exception": false,
     "start_time": "2024-01-17T22:31:26.284282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM generation text detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe039d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T22:31:26.298997Z",
     "iopub.status.busy": "2024-01-17T22:31:26.298575Z",
     "iopub.status.idle": "2024-01-17T22:31:50.328877Z",
     "shell.execute_reply": "2024-01-17T22:31:50.327833Z"
    },
    "papermill": {
     "duration": 24.038145,
     "end_time": "2024-01-17T22:31:50.331518",
     "exception": false,
     "start_time": "2024-01-17T22:31:26.293373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import keras_nlp\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import string\n",
    "\n",
    "\n",
    "train_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "train_prompts = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")\n",
    "train_daigt = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40085ce",
   "metadata": {
    "papermill": {
     "duration": 0.004202,
     "end_time": "2024-01-17T22:31:50.340441",
     "exception": false,
     "start_time": "2024-01-17T22:31:50.336239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4394ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T22:31:50.351305Z",
     "iopub.status.busy": "2024-01-17T22:31:50.350444Z",
     "iopub.status.idle": "2024-01-17T22:31:50.407404Z",
     "shell.execute_reply": "2024-01-17T22:31:50.406455Z"
    },
    "papermill": {
     "duration": 0.064966,
     "end_time": "2024-01-17T22:31:50.409729",
     "exception": false,
     "start_time": "2024-01-17T22:31:50.344763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_daigt[\"generated\"] = train_daigt[\"label\"]\n",
    "train_essays = train_essays.merge(train_prompts, on='prompt_id', how='inner')\n",
    "\n",
    "train_essays = pd.concat([train_essays,train_daigt])\n",
    "\n",
    "#train_essays.loc[:,[\"prompt_name\",\"generated\"]].value_counts()\n",
    "train_essays = train_essays.sample(frac = 1)\n",
    "\n",
    "train_essays = train_essays.loc[:,[\"text\",\"generated\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18c116",
   "metadata": {
    "papermill": {
     "duration": 0.004129,
     "end_time": "2024-01-17T22:31:50.418148",
     "exception": false,
     "start_time": "2024-01-17T22:31:50.414019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9239acd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T22:31:50.428218Z",
     "iopub.status.busy": "2024-01-17T22:31:50.427855Z",
     "iopub.status.idle": "2024-01-17T22:54:24.270288Z",
     "shell.execute_reply": "2024-01-17T22:54:24.269012Z"
    },
    "papermill": {
     "duration": 1353.854753,
     "end_time": "2024-01-17T22:54:24.277144",
     "exception": false,
     "start_time": "2024-01-17T22:31:50.422391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352.5375156402588\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "table[10] = None #\\n\n",
    "table[92] = None #\\\n",
    "\n",
    "\n",
    "#to lower\n",
    "for code in range(26):\n",
    "    table[code + 65] = code +97\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    #characters level preprocessing\n",
    "    #remove \\n and \\, remove puntuactions, to lower case\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    # Tokenization using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Remove stopwords and lemmization using spaCy's built-in stopword list\n",
    "    tokens = [token.lemma_ for token in doc if not nlp.vocab[token.text].is_stop]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Number of parallel processes (adjust according to your CPU cores)\n",
    "num_processes = 4\n",
    "\n",
    "# Split the DataFrame into chunks\n",
    "chunks = np.array_split(train_essays.loc[:,\"text\"], num_processes)\n",
    "\n",
    "# Function to apply to each chunk in parallel\n",
    "def parallel_map(chunk):\n",
    "    return chunk.map(lambda x : preprocess_text(x))\n",
    "\n",
    "# Initialize a Pool for parallel processing\n",
    "with Pool(num_processes) as pool:\n",
    "    # Use map function to apply the parallel_map function to each chunk\n",
    "    results = pool.map(parallel_map, chunks)\n",
    "\n",
    "# Concatenate the results back into a single DataFrame\n",
    "train_essays[\"text\"] = pd.concat(results, axis=0)\n",
    "\n",
    "#train_essays.loc[:,\"text\"] = train_essays.loc[:,\"text\"].map(lambda x : preprocess_text(x))\n",
    "# Convert back to pandas DataFrame (if needed)\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83998906",
   "metadata": {
    "papermill": {
     "duration": 0.004356,
     "end_time": "2024-01-17T22:54:24.286250",
     "exception": false,
     "start_time": "2024-01-17T22:54:24.281894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3abdd73",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-17T22:54:24.297310Z",
     "iopub.status.busy": "2024-01-17T22:54:24.296943Z",
     "iopub.status.idle": "2024-01-17T22:54:59.898204Z",
     "shell.execute_reply": "2024-01-17T22:54:59.897282Z"
    },
    "papermill": {
     "duration": 35.609985,
     "end_time": "2024-01-17T22:54:59.900755",
     "exception": false,
     "start_time": "2024-01-17T22:54:24.290770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'generated'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/bert/keras/bert_large_en_uncased/1' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_backbone\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " token_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_embedding (Reversibl  (None, None, 1024)           3125452   ['token_ids[0][0]']           \n",
      " eEmbedding)                                              8                                       \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)    [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " position_embedding (Positi  (None, None, 1024)           524288    ['token_embedding[0][0]']     \n",
      " onEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " segment_embedding (Embeddi  (None, None, 1024)           2048      ['segment_ids[0][0]']         \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, 1024)           0         ['token_embedding[0][0]',     \n",
      "                                                                     'position_embedding[0][0]',  \n",
      "                                                                     'segment_embedding[0][0]']   \n",
      "                                                                                                  \n",
      " embeddings_layer_norm (Lay  (None, None, 1024)           2048      ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " embeddings_dropout (Dropou  (None, None, 1024)           0         ['embeddings_layer_norm[0][0]'\n",
      " t)                                                                 ]                             \n",
      "                                                                                                  \n",
      " padding_mask (InputLayer)   [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " transformer_layer_0 (Trans  (None, None, 1024)           1259622   ['embeddings_dropout[0][0]',  \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_1 (Trans  (None, None, 1024)           1259622   ['transformer_layer_0[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_2 (Trans  (None, None, 1024)           1259622   ['transformer_layer_1[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_3 (Trans  (None, None, 1024)           1259622   ['transformer_layer_2[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_4 (Trans  (None, None, 1024)           1259622   ['transformer_layer_3[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_5 (Trans  (None, None, 1024)           1259622   ['transformer_layer_4[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_6 (Trans  (None, None, 1024)           1259622   ['transformer_layer_5[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_7 (Trans  (None, None, 1024)           1259622   ['transformer_layer_6[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_8 (Trans  (None, None, 1024)           1259622   ['transformer_layer_7[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_9 (Trans  (None, None, 1024)           1259622   ['transformer_layer_8[0][0]', \n",
      " formerEncoder)                                           4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_10 (Tran  (None, None, 1024)           1259622   ['transformer_layer_9[0][0]', \n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_11 (Tran  (None, None, 1024)           1259622   ['transformer_layer_10[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_12 (Tran  (None, None, 1024)           1259622   ['transformer_layer_11[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_13 (Tran  (None, None, 1024)           1259622   ['transformer_layer_12[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_14 (Tran  (None, None, 1024)           1259622   ['transformer_layer_13[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_15 (Tran  (None, None, 1024)           1259622   ['transformer_layer_14[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_16 (Tran  (None, None, 1024)           1259622   ['transformer_layer_15[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_17 (Tran  (None, None, 1024)           1259622   ['transformer_layer_16[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_18 (Tran  (None, None, 1024)           1259622   ['transformer_layer_17[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_19 (Tran  (None, None, 1024)           1259622   ['transformer_layer_18[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_20 (Tran  (None, None, 1024)           1259622   ['transformer_layer_19[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_21 (Tran  (None, None, 1024)           1259622   ['transformer_layer_20[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_22 (Tran  (None, None, 1024)           1259622   ['transformer_layer_21[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_layer_23 (Tran  (None, None, 1024)           1259622   ['transformer_layer_22[0][0]',\n",
      " sformerEncoder)                                          4          'padding_mask[0][0]']        \n",
      "                                                                                                  \n",
      " pooled_dense (Dense)        (None, None, 1024)           1049600   ['transformer_layer_23[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 1024)                 0         ['pooled_dense[0][0]']        \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335141888 (1.25 GB)\n",
      "Trainable params: 335141888 (1.25 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bert_kernel_initializer(stddev=0.02):\n",
    "    return keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "print(train_essays.columns)\n",
    "\n",
    "X = train_essays[\"text\"].values\n",
    "y = train_essays[\"generated\"].values\n",
    "\n",
    "\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Assuming your labels are 0 and 1\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "hidden_size = 1024\n",
    "dropout_prob = 0.3\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "#val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "#val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_large_en_uncased\")\n",
    "\n",
    "train_preprocessed = (\n",
    "    train_dataset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\"\"\"\n",
    "val_preprocessed = (\n",
    "    val_dataset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Backbone\n",
    "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_large_en_uncased\")\n",
    "\n",
    "backbone.summary()\n",
    "\n",
    "backbone.trainable = False\n",
    "inputs = backbone.input\n",
    "\n",
    "pooled = backbone(inputs)[\"pooled_output\"]\n",
    "\n",
    "x = keras.layers.Dense(hidden_size)(pooled)\n",
    "x = keras.layers.Dense(hidden_size, activation = 'relu')(x)\n",
    "x = keras.layers.Dropout(dropout_prob)(x)\n",
    "x = keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "x = keras.layers.Dense(hidden_size,  activation='relu')(x)\n",
    "x = keras.layers.Dropout(dropout_prob)(x)\n",
    "x = keras.layers.Dense(hidden_size//8, activation = 'relu')(x)\n",
    "\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715b6fc",
   "metadata": {
    "papermill": {
     "duration": 0.014573,
     "end_time": "2024-01-17T22:54:59.930103",
     "exception": false,
     "start_time": "2024-01-17T22:54:59.915530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbf10bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T22:54:59.961936Z",
     "iopub.status.busy": "2024-01-17T22:54:59.961569Z",
     "iopub.status.idle": "2024-01-18T05:28:21.865055Z",
     "shell.execute_reply": "2024-01-18T05:28:21.863866Z"
    },
    "papermill": {
     "duration": 23602.560084,
     "end_time": "2024-01-18T05:28:22.505385",
     "exception": false,
     "start_time": "2024-01-17T22:54:59.945301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "723/723 [==============================] - 2447s 3s/step - loss: 0.6682 - auc: 0.6246\n",
      "Epoch 2/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.5638 - auc: 0.7793\n",
      "Epoch 3/10\n",
      "723/723 [==============================] - 2348s 3s/step - loss: 0.5243 - auc: 0.8155\n",
      "Epoch 4/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4982 - auc: 0.8361\n",
      "Epoch 5/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4919 - auc: 0.8412\n",
      "Epoch 6/10\n",
      "723/723 [==============================] - 2350s 3s/step - loss: 0.4725 - auc: 0.8553\n",
      "Epoch 7/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4673 - auc: 0.8588\n",
      "Epoch 8/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4624 - auc: 0.8620\n",
      "Epoch 9/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4549 - auc: 0.8670\n",
      "Epoch 10/10\n",
      "723/723 [==============================] - 2349s 3s/step - loss: 0.4539 - auc: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x79a74a31ddb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch_sizes = [64,32]\n",
    "learning_rates = [4e-4,3e-3,5e-5]\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    \n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_preprocessed = (\n",
    "        train_dataset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_preprocessed = (\n",
    "        val_dataset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    for lr in learning_rates:\n",
    "        print(\"EXECUTION BATCH SIZE {} LR {}\".format(batch_size,lr))\n",
    "\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.AdamW(lr),\n",
    "            metrics = [keras.metrics.AUC()],\n",
    "            jit_compile=True,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_preprocessed,\n",
    "            validation_data=val_preprocessed,\n",
    "            epochs=2,\n",
    "            class_weight = {0:class_weights[0],1:class_weights[1]}\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            optimizer=keras.optimizers.AdamW(2e-4),\n",
    "            metrics = [keras.metrics.AUC()],\n",
    "            jit_compile=True,\n",
    "        )\n",
    "\n",
    "model.fit(\n",
    "    train_preprocessed,\n",
    "    epochs=10,\n",
    "    class_weight = {0:class_weights[0],1:class_weights[1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e32d4b",
   "metadata": {
    "papermill": {
     "duration": 0.654203,
     "end_time": "2024-01-18T05:28:23.829692",
     "exception": false,
     "start_time": "2024-01-18T05:28:23.175489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7f8229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T05:28:25.145378Z",
     "iopub.status.busy": "2024-01-18T05:28:25.144496Z",
     "iopub.status.idle": "2024-01-18T05:28:35.726305Z",
     "shell.execute_reply": "2024-01-18T05:28:35.725402Z"
    },
    "papermill": {
     "duration": 11.241248,
     "end_time": "2024-01-18T05:28:35.728927",
     "exception": false,
     "start_time": "2024-01-18T05:28:24.487679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
      "array([[  101, 13360, 22861, ...,     0,     0,     0],\n",
      "       [  101, 22861,  2497, ...,     0,     0,     0],\n",
      "       [  101, 10507,  2278, ...,     0,     0,     0]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(3, 512), dtype=bool, numpy=\n",
      "array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]])>}\n",
      "1/1 [==============================] - 10s 10s/step\n"
     ]
    }
   ],
   "source": [
    "test_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\n",
    "test_essays.loc[:,\"text\"] = test_essays.loc[:,\"text\"].map(lambda x : preprocess_text(x))\n",
    "print(preprocessor(test_essays[\"text\"]))\n",
    "submission_dict  = pd.DataFrame()\n",
    "submission_dict[\"id\"] = test_essays[\"id\"]\n",
    "submission_dict[\"generated\"] = model.predict(preprocessor(test_essays[\"text\"]))[:,0]\n",
    "\n",
    "\n",
    "submission = pd.DataFrame.from_dict(submission_dict)\n",
    "submission.to_csv(\"/kaggle/working/submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 7064285,
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7237800,
     "modelInstanceId": 4682,
     "sourceId": 5909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 7429201,
     "modelInstanceId": 4682,
     "sourceId": 6061,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25037.536422,
   "end_time": "2024-01-18T05:28:40.130700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-17T22:31:22.594278",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
