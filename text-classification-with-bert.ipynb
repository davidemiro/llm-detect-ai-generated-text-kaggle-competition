{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":5909,"sourceType":"modelInstanceVersion","modelInstanceId":4682},{"sourceId":6061,"sourceType":"modelInstanceVersion","modelInstanceId":4682}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM generation text detection ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nimport keras_nlp\nimport re\nimport spacy\nimport time\nimport string\n\n\ntrain_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\ntrain_prompts = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")\ntrain_daigt = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:49:37.019094Z","iopub.execute_input":"2024-01-11T22:49:37.019536Z","iopub.status.idle":"2024-01-11T22:50:00.855766Z","shell.execute_reply.started":"2024-01-11T22:49:37.019504Z","shell.execute_reply":"2024-01-11T22:50:00.854964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets ","metadata":{}},{"cell_type":"code","source":"train_daigt[\"generated\"] = train_daigt[\"label\"]\ntrain_essays = train_essays.merge(train_prompts, on='prompt_id', how='inner')\n\ntrain_essays = pd.concat([train_essays,train_daigt])\n\n#train_essays.loc[:,[\"prompt_name\",\"generated\"]].value_counts()\ntrain_essays = train_essays.sample(frac = 1)\n\ntrain_essays = train_essays.loc[:,[\"text\",\"generated\"]]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:51:39.426153Z","iopub.execute_input":"2024-01-11T22:51:39.427025Z","iopub.status.idle":"2024-01-11T22:51:39.475455Z","shell.execute_reply.started":"2024-01-11T22:51:39.426993Z","shell.execute_reply":"2024-01-11T22:51:39.474707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load the English NLP model\nnlp = spacy.load(\"en_core_web_sm\")\n\ntable = str.maketrans(\"\", \"\", string.punctuation)\ntable[10] = None #\\n\ntable[92] = None #\\\n\n\n#to lower\nfor code in range(26):\n    table[code + 65] = code +97\n\n\n\n\ndef preprocess_text(text):\n    \n    #characters level preprocessing\n    #remove \\n and \\, remove puntuactions, to lower case\n    text = text.translate(table)\n    \n    # Tokenization using spaCy\n    doc = nlp(text)\n    \n    # Remove stopwords and lemmization using spaCy's built-in stopword list\n    tokens = [token.lemma_ for token in doc if not nlp.vocab[token.text].is_stop]\n    \n    return \" \".join(tokens)\n\n\n\n\nstart_time = time.time()\ntrain_essays.loc[:,\"text\"] = train_essays.loc[:,\"text\"].map(lambda x : preprocess_text(x))\n# Convert back to pandas DataFrame (if needed)\nend_time = time.time()\n\nprint(end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:51:41.853624Z","iopub.execute_input":"2024-01-11T22:51:41.853976Z","iopub.status.idle":"2024-01-11T22:51:43.584421Z","shell.execute_reply.started":"2024-01-11T22:51:41.853948Z","shell.execute_reply":"2024-01-11T22:51:43.583343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split\n\nX = train_essays[\"text\"].values\ny = train_essays[\"generated\"].values\n\n\n#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.0, stratify=y, random_state=42)\n\n# Assuming your labels are 0 and 1\nclass_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_large_en_uncased\",\n    num_classes=1,\n)\n\n\n\n# Access backbone programmatically (e.g., to change `trainable`).\nclassifier.backbone.trainable = False\n\n#grid search\n\n\n\nclassifier.compile(\n    loss = keras.losses.BinaryCrossentropy(),\n    metrics=[keras.metrics.AUC()],\n    optimizer = keras.optimizers.Adam(4e-4),\n    jit_compile = True,\n)\n\n\n\nhistory = classifier.fit(\n    x = X,\n    y = y,\n    batch_size = 64,\n    epochs = 10,  # Set the number of epochs as required\n    class_weight = {0:class_weights[0],1:class_weights[1]}\n)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T22:51:56.816946Z","iopub.execute_input":"2024-01-11T22:51:56.817335Z","iopub.status.idle":"2024-01-11T22:53:14.193529Z","shell.execute_reply.started":"2024-01-11T22:51:56.817303Z","shell.execute_reply":"2024-01-11T22:53:14.192528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation and submission","metadata":{}},{"cell_type":"code","source":"test_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\nsubmission_dict  = pd.DataFrame()\n\nsubmission_dict[\"id\"] = test_essays[\"id\"]\nsubmission_dict[\"generated\"] = np.around(classifier.predict(test_essays[\"text\"])[:,0],1)\n\n\nsubmission = pd.DataFrame.from_dict(submission_dict)\nsubmission.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}